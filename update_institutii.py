import requests
from bs4 import BeautifulSoup
import pandas as pd
import os

# 1. Pagina cu linkurile cÄƒtre fiÈ™ierele Excel
pagina_url = "https://extranet.anaf.mfinante.gov.ro/anaf/extranet/EXECUTIEBUGETARA/alte_rapoarte/alte_rapoarte2"

# 2. CiteÈ™te HTML-ul paginii
headers = {"User-Agent": "Mozilla/5.0"}
r = requests.get(pagina_url, headers=headers)
soup = BeautifulSoup(r.content, "html.parser")

# 3. CautÄƒ primul link care conÈ›ine .xls sau .xlsx
link_excel = None
for a in soup.find_all("a", href=True):
    href = a["href"]
    if href.endswith(".xls") or href.endswith(".xlsx"):
        if not href.startswith("http"):
            href = "https://extranet.anaf.mfinante.gov.ro" + href
        link_excel = href
        break

if not link_excel:
    raise Exception("Nu am gÄƒsit linkul cÄƒtre fiÈ™ierul Excel!")

print(f"ğŸ“ Link gÄƒsit: {link_excel}")

# 4. DescarcÄƒ fiÈ™ierul Excel
r_excel = requests.get(link_excel)
with open("institutii.xlsx", "wb") as f:
    f.write(r_excel.content)

# 5. ÃncarcÄƒ Excelul Ã®n Pandas È™i converteÈ™te Ã®n JSON
df = pd.read_excel("institutii.xlsx")

# ğŸ”§ (opÈ›ional) SelecteazÄƒ doar coloanele relevante
# df = df[["Denumire", "Cod fiscal", "JudeÈ›", "Tip instituÈ›ie"]]

# ÃnlocuieÈ™te NaN cu È™iruri goale È™i asigurÄƒ-te cÄƒ toate coloanele sunt string
df = df.fillna("").astype(str)

# SalveazÄƒ ca JSON
df.to_json("institutii.json", orient="records", force_ascii=False)

print("âœ… institutii.json generat cu succes.")
